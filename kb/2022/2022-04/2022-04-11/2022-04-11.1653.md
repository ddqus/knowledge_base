# Benchmarking Apache Kafka: 2 Million Writes Per Second

```
google:kafka benchmark
#kafka #benchmark
```

* 2014년 기준, (날짜를 봐서는 0.8.1 이후 버전을 사용한듯)
* 테스트 머신: 아래 사양으로 6대
    * Intel Xeon 2.5 GHz processor with six cores
    * Six 7200 RPM SATA drives
    * 32GB of RAM
    * 1Gb Ethernet
* 3대로 카프카 클러스터 구성, 하드 6개는 RAID 아님.
* 나머지 3대는 zookeeper, 부하 생성용
* consumer 는 없음. producer 가 생성만 함. (즉 read 는 없고 write 만 함)


* 시나리오1: Single producer thread, no replication
    * topic 1개, 파티션 6개, no repl
    * 싱글 스레드로, 순차적으로 100바이트 small 메시지 5천만개 발행
    * small 메시지인 이유: 테스트의 목적 = harder case 테스트
        * 메시지 크기가 크면 당연히 MB/sec 가 높을 것이다. 크기가 작을수록 overhead 비율이 커진다.
    * 메시지가 100바이트면 overhead 는 약 22바이트 정도 된다.
        * for an optional key, size delimiting, a message CRC, the record offset, and attributes flag
        * the topic, partition, required acknowledgements, etc
    * 결과: 821,557 records/sec (78.3 MB/sec)


* 시나리오2: Single producer thread, 3x asynchronous replication
    * 테스트 환경은 시나리오1과 동일하다. 다만 각각의 파티션은 3개의 repl 을 가진다.
        * (그러니 네트워크와 디스크의 데이터 양은 총 3배 더 많아질 것이다)
    * repl 이 생겼으니, 서버들은 produce 메시지 저장 뿐만 아니라 repl 동기화를 위한 작업을 추가로 해야한다.
    * repl 은 비동기 모드로 설정하였다.
        * leader broker 가 local 에 저장한 후 다른 repl 이 복제하기 전에 응답을 끝낸다.
        * 데이터 손실 위험이 있지만(leader crash) 응답속도가 개선된다.
    * 결과: 786,980 records/sec (75.1 MB/sec)
        * 카프카 클러스터 내의 데이터 저장량은 3배로 늘었다.
            * repl 이 3개이니 한번의 저장이 3번 반복되어야 한다.
        * 3배로 늘었음에도 불구하고 처리량은 시나리오1과 비교했을때 여전히 좋다.


* 시나리오3: Single producer thread, 3x synchronous replication
    * 테스트 환경은 시나리오2와 동일하다. 차이점은, repl 을 동기 모드로 설정하였다.
        * leader broker 가 local 에 저장한 후 모든 repl 이 복제하기 까지 기다리고 응답한다.
    * 결과: 421,823 records/sec (40.2 MB/sec)
        * 모든 repl 이 복제하기 까지 응답시간이 지연된다.
        * 그래서 처리량이 거의 50% 줄었다.


* 시나리오4: Three producers, 3x async replication
    * 시나리오1,2,3 은 1개의 producer 를 사용했는데, 부하를 최대치로 생성하지 못했다.
    * 그래서 (kafka broker가 아닌) 3대의 서버로 producer 를 구성해서 부하를 더 늘렸다.
    * 결과: 2,024,032 records/sec (193.0 MB/sec)
        * 3배로 늘었다.


* 시나리오5: Producer Throughput Versus Stored Data
    * 메시징 시스템에서 놓치기 쉬운 위험사항이 있다.
    * 메시지를 받으면 일단은 메모리에 캐싱되어 있을 것이다.
    * 만약 consume 속도가 produce 속도보다 느리다면? lag 이 발생한다.
    * lag 이 계속 늘어난다면? 메모리 용량을 초과하는 메시지는 디스크에 저장되어야 한다.
        * 이런 데이터를 접근할 경우 디스크 IO 가 발생할 것이고, 디스크 IO 는 성능에 심각한 악영향을 미친다.
    * 이런 경우를 테스트 하려면 어떻게 해야 하나? 메모리 용량을 넘어가는 대용량 메시지를 produce 해본다.
        * TB 수준으로 생성한다.
    * 결과: 문제 없음
        * 데이터 용량은 성능에 영향을 주지 않았다.


* 이제 consumer 테스트를 해보자
    * replication 설정은 consumer 테스트에는 영향을 주지 않는다.
    * 왜냐, consumer 는 하나의 replica 에서만 읽어가기 때문이다.


* 시나리오6: Single Consumer
    * 싱글 스레드 consumer, 파티션 6개, replication 3x, 메시지 5천만개
    * 결과: 940,521 records/sec (89.7 MB/sec)


* 시나리오7: Three Consumers
    * 성능 3배 증가, 선형 증가
    * 결과: 2,615,968 records/sec, (249.5 MB/sec)


* 시나리오8: Producer and Consumer
    * producer 와 consumer 를 같이 테스트 한다.
    * 환경은 동일, 파티션 6개, replication 3x, async replication
    * 결과는 consumer 기준이다.
    * 결과: 795,064 records/sec, (75.8 MB/sec)
        * 시나리오2와 비슷하다.
        * 즉 consumer 사용 비용은 매우 가볍다는 것이다.


* Effect of Message Size
    * 위의 테스트의 메시지 크기는 100 바이트이다.
    * 만약 메시지 크기가 늘어나면 성능에 어떤 영향을 줄 것인가? 100 kb 라면?
    * 당연히 메시지 크기가 늘어나면 처리량(records per sec)은 낮아질 것이다.
    * 하지만 처리량을 갯수가 아닌 용량으로 볼 경우, 메시지 크기가 커지면 전체 처리용량은 계속 증가한다.
        * 즉 비효율적이 아니라는 것이다.


* End-to-end Latency
    * 2 ms (median)
    * 3 ms (99th percentile)
    * 14 ms (99.9th percentile)
    * 매우 신뢰성 높고 빠름

> https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines
>
> setup: https://gist.github.com/jkreps/c7ddb4041ef62a900e6c


benchmark-commands.txt

```shell
# Producer

# Setup
bin/kafka-topics.sh --zookeeper esv4-hcl197.grid.linkedin.com:2181 --create --topic test-rep-one --partitions 6 --replication-factor 1
bin/kafka-topics.sh --zookeeper esv4-hcl197.grid.linkedin.com:2181 --create --topic test --partitions 6 --replication-factor 3

# Single thread, no replication

bin/kafka-run-class.sh org.apache.kafka.clients.tools.ProducerPerformance test7 50000000 100 -1 acks=1 \
  bootstrap.servers=esv4-hcl198.grid.linkedin.com:9092 buffer.memory=67108864 batch.size=8196

# Single-thread, async 3x replication

bin/kafktopics.sh --zookeeper esv4-hcl197.grid.linkedin.com:2181 --create --topic test --partitions 6 --replication-factor 3
bin/kafka-run-class.sh org.apache.kafka.clients.tools.ProducerPerformance test6 50000000 100 -1 acks=1 \
  bootstrap.servers=esv4-hcl198.grid.linkedin.com:9092 buffer.memory=67108864 batch.size=8196

# Single-thread, sync 3x replication

bin/kafka-run-class.sh org.apache.kafka.clients.tools.ProducerPerformance test 50000000 100 -1 acks=-1 \
  bootstrap.servers=esv4-hcl198.grid.linkedin.com:9092 buffer.memory=67108864 batch.size=64000

# Three Producers, 3x async replication
bin/kafka-run-class.sh org.apache.kafka.clients.tools.ProducerPerformance test 50000000 100 -1 acks=1 \
  bootstrap.servers=esv4-hcl198.grid.linkedin.com:9092 buffer.memory=67108864 batch.size=8196

# Throughput Versus Stored Data

bin/kafka-run-class.sh org.apache.kafka.clients.tools.ProducerPerformance test 50000000000 100 -1 acks=1 \
  bootstrap.servers=esv4-hcl198.grid.linkedin.com:9092 buffer.memory=67108864 batch.size=8196

# Effect of message size

for i in 10 100 1000 10000 100000;
do
echo ""
echo $i
bin/kafka-run-class.sh org.apache.kafka.clients.tools.ProducerPerformance test $((1000*1024*1024/$i)) $i -1 acks=1 \
  bootstrap.servers=esv4-hcl198.grid.linkedin.com:9092 buffer.memory=67108864 batch.size=128000
done;

# Consumer
# Consumer throughput

bin/kafka-consumer-perf-test.sh --zookeeper esv4-hcl197.grid.linkedin.com:2181 --messages 50000000 --topic test --threads 1

# 3 Consumers

# On three servers, run:
bin/kafka-consumer-perf-test.sh --zookeeper esv4-hcl197.grid.linkedin.com:2181 --messages 50000000 --topic test --threads 1

# End-to-end Latency

bin/kafka-run-class.sh kafka.tools.TestEndToEndLatency esv4-hcl198.grid.linkedin.com:9092 esv4-hcl197.grid.linkedin.com:2181 test 5000

# Producer and consumer

bin/kafka-run-class.sh org.apache.kafka.clients.tools.ProducerPerformance test 50000000 100 -1 acks=1 \
  bootstrap.servers=esv4-hcl198.grid.linkedin.com:9092 buffer.memory=67108864 batch.size=8196

bin/kafka-consumer-perf-test.sh --zookeeper esv4-hcl197.grid.linkedin.com:2181 --messages 50000000 --topic test --threads 1
```

server-config.properties

```
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# see kafka.server.KafkaConfig for additional details and defaults

############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=0

############################# Socket Server Settings #############################

# The port the socket server listens on
port=9092

# Hostname the broker will bind to and advertise to producers and consumers.
# If not set, the server will bind to all interfaces and advertise the value returned from
# from java.net.InetAddress.getCanonicalHostName().
#host.name=localhost

# The number of threads handling network requests
num.network.threads=4
 
# The number of threads doing disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=1048576

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=1048576

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


############################# Log Basics #############################

# The directory under which to store log files
log.dirs=/grid/a/dfs-data/kafka-logs,/grid/b/dfs-data/kafka-logs,/grid/c/dfs-data/kafka-logs,/grid/d/dfs-data/kafka-logs,/grid/e/dfs-data/kafka-logs,/grid/f/dfs-data/kafka-logs

# The number of logical partitions per topic per server. More partitions allow greater parallelism
# for consumption, but also mean more files.
num.partitions=8

############################# Log Flush Policy #############################

# The following configurations control the flush of data to disk. This is the most
# important performance knob in kafka.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data is at greater risk of loss in the event of a crash.
#    2. Latency: Data is not made available to consumers until it is flushed (which adds latency).
#    3. Throughput: The flush is generally the most expensive operation. 
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# Per-topic overrides for log.flush.interval.ms
#log.flush.intervals.ms.per.topic=topic1:1000, topic2:3000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
# segments don't drop below log.retention.bytes.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=536870912

# The interval at which log segments are checked to see if they can be deleted according 
# to the retention policies
log.cleanup.interval.mins=1

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=esv4-hcl197.grid.linkedin.com:2181

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=1000000

# metrics reporter properties
kafka.metrics.polling.interval.secs=5
kafka.metrics.reporters=kafka.metrics.KafkaCSVMetricsReporter
kafka.csv.metrics.dir=/tmp/kafka_metrics
# Disable csv reporting by default.
kafka.csv.metrics.reporter.enabled=false

replica.lag.max.messages=10000000

```